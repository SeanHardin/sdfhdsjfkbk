book for class: Machine Learning Tom Mitchell

grading
projects 40% 4 for 10 each
midterm 30%
non-cumulative final 30%

machine learning - the study of algorithms that improves their performance P at some task T with experience E.
p - desired outcome
T - task
E - given data
performance is related to the desired outcome of an event. ie. win chess, accurately diagnose, identify groups.

supervised learning - training data includes expected output
unsupervised learning - training data does not include expected output.



attributes - features
prediction - class (label)
preprocessing is often needed.
x1, x2 etc - a single training example
X: - set of all possible combinations of features
F:X->Y - target concept
H: - set of all hypotheses that the learner may consider regarding the identity of F
goal of the learner - find h within H such that h(x) == f(x) (the right hypothesis) for all xi in X.
if y is discrete, it is called a classification task
if y is continuous, it is called a regression task

model representations - supervised learning
 decision trees
 neural networks
 support vector machines (SVM)
 Bayesian networks

representations with unsupervised learning - discover pattern in the data
 data without labels?
 clustering
 

probability = # times event occurs/total trials
sample space - set of all possible outcomes of an event
P(a|B) = P(A && B)/P(B)
P(x1,x2,x3,x4) = P(x1|others)P(x2|x3,x4)P(x3|x4)P(x4)
P(A || B) = P(A) + P(B) - P(A,B)
gradient descent better when used with higher dimensional space.
 minimize sum of squared errors..... derive SSE/2, calculate partial derivative with M, get vector.
 something something put a negative on the vector to get the steepest decrease..?
  gradient of F(x,y) = [partial deriv x, partial deriv y] if f(x,y) = x+3y^2, gradient = [1,6y]
   so applying a point to the negative of the gradient, you get closer to minimum.
   
sum of squared error = the sum((actualData - predictedValue)^2)

concept learning - infer a concept by learning from past experiences
inductive learning hypothesis - READ CHAPTER 2 BECAUSE WHAT THE HELL IS THIS.

attributes have various values. introduce new value ? meaning that any value is okay.

given hi and hj, hi is more general than or rqual to hj (hi >= hj) if and only if all possibilities of hj are contained within hi...........
A ? ?   h1
? B ?   h3
h1 !>= h3, h3 !>= h1

find-s algorithm - make a hypothesis to match the first data point, then check every other positive data point and generalize
 the hypothesis with each one. this is a pretty bad algorithm.
 it can predict known failures as positive.
  this algorithm finds the h: <sunny, warm, ?, Strong, ?, ?>, which could be one of 6. itself, one of the 3 pairs, or one of the singles.
 if 2 of 3 options are found, then no hypothesis can explain the data. BUT A ? IS STILL USED TO SHOW IT. THATS STUPID.
 conditions - there are no training errors (aka impossible on real data), there exists a hypothesis that can explains the target concept
 weaknesses - only finds single hypothesis when there could be more, breaks if training errors exist, only finds single hypothesis

candidate elimination algorithm - finds all potential hypotheses that can match the data?
 version space - VSh,d = {h in H | consistent(h,D)} the version space is the set of all hypotheses consistent with training data.
 start with the empty set as the marker for most specific, all ? hypothesis for most general.
  most specific will follow find-S algorithm, 
  most general will include a number of hypotheses, each focusing on a single attribute's value. the possibilities are then limited
   based on what is allowed from the most specific hypothesis. constant checking between them is needed.
   <sunny,warm,?,strong,warm,same> is S3
   <sunny ?????>, <?warm?????><?????same> is G
    next iteration changes same from s3 into a ?, thus the third hypothesis in G must be removed because it is now inconsistent with the data.
 conditions - there can not be any training errors, can be explained by a set of hypotheses.
 using results - all yes, positive.   majority yes, positive, even, need more data, more fail, negative.

decision tree - learning by inductive inference that is much more widely used than what we covered in concept leaning
 robust to noisy data, search a more expressive hypothesis space.
 learned function is represented as a decision tree, classification label usually discrete values, can be continuous
 used to classify patients by disease, find cause of equipment malfunctions, assess credit risk of loan applicants.
core algorithm known as ID3 (uses entropy and information gain to construct tree)
 'intuition' explains that the more an attribute is able to determine the classification outcome by itself, the higher up it should be.
  information gain is the measure of how well a given attribute determines the classification.
  entropy is used to measure the degree of similarity
   entropy - how pure something is. 
Entropy(s) = -P+*log(P+) - P-*log(P-)    P+ = probability value is positive.  log is base 2.
    min = 0 at only pos/neg.   max = 1 at 50% each
information gain - measures reduction in entropy caused by partitioning training examples based on A
gain(S,A) = Entropy(S) - SIGMA(v in Values(A))(|Sv|*Entropy(Sv)/|S|) this formula is a fucking mess...
          = Entropy(S) - (Sv1/S*Entropy(Sv) + 
 
gain(S,wind)=entropy(S)-(|Sweak|/|S| * entropy(Sweak) + |Sstrong|/|S| * entropy (Sstrong))
 14 entries, strong/yes 3   strong/no 3   weak/yes 6  weak/no 2  
 total strong 6  total weak 8  total yes 9   total no 5

 entropy(S) = -9/14*log(9/14) - 5/14 * log(5/14) = .94ish
 gain = .94 - (6/14 * entropy(Sstrong) + 8/14 * entropy(Sweak))
		      -3/6 * log(3/6) - 3/6*log(3/6)          -6/8*log(6/8) - 2/8*log(2/8)
 = .94 - (6/14*(-3/6 * log(3/6) - 3/6*log(3/6)) + 8/14*( -6/8*log(6/8) - 2/8*log(2/8)))

weakness - doesn't backtrack, but does it need to?

S = sample space of given attribute
A = set potential values of attribute

Entropy(S) = -(prob pos (total yes/total))*log(prob pos)) - (prob neg(total no/total))*log(prob neg))
gain(S,A) = Entropy(S) - {prob S has Ai(total strong/total) * Entropy(S with Ai)} SUMMED FOR ALL Ai
 Entropy(S with Ai) = -(prob+((strong/yes / total strong)3/6)log(prob+(3/6)) -(prob-(3/6)log(prob-(3/6))

entropy(S) = total entropy in table
gain(S,A) = current entropy minus the sum of the entropy found within each possible value of the attribute
 if the entropy within the sum of potential values is significantly smaller than the original entropy, then they should be split up to
  reduce the total entropy in the table. THE VALUE WITH THE HIGHEST GAIN VALUE IS THE BEST OPTION FOR THE ROOT NODE OF A SPLIT.


After going down a branch, S becomes SAi (ie Ssunny on the sunny path)

inductive bias - assumptions regarding the hypothesis and training data allow us to generalize over unseen examples.
bias favors smaller trees?
approximate bias - shorter trees are preferred over larger ones.
Occam's razor - choosing the simplest way to describe why something happens when multiple hypotheses are present.

with training data, overfitting increases accuracy (for obvious reasons)
with real data, accuracy falls once too many nodes are made. peaks at 5, drops around 20, progressively gets worse (in one study)

binning - giving ranges for continuous values to partition them into a manageable number of potential values.
strategies to fill in blank values - 
 choose most common value for that attribute
 choose most common value for that attribute and class label (y/n)
 fill in as probabilities based on previous values.

k nearest neighbor - save training data, classify new instance based on classification of subset of training instances that the new one
 is 'most near' assumes nearness indicates similarity
 measure nearness - d(xi,xj) = (sum((ar*xi - ar*xj)^2) from 1 to n)^(1/2)
essentially, finding the nearest n points, describes the newly added point as the color that is most common among the nearest n previous ones
 in the case of numbers, it would be the mean of them
 k must be odd so that ties dont happen. but they still can if theres 3 types...

 for weighted k nearest neighbor, multiply the distance by the weight
 weight = 1/(d(xq,xi)^2)
as k increases, noise has less of an effect.

advantages - intuitive and easy to understand
 estimate of target function is local and different for every sample
 robust to noisy data, good classificatio if data is suficiently large

disadvantages - cost of classification is high because computation takes place at classification time
 all attributes are used when determining what samples to pick
 determining the optimum k can be tricky.

curse of dimensionality - distance between instances is calculated based on all attributes of the instance. this is in contrast to other learning algorithms
 this can be bad. for instance, with 20 attributes where only 2 apply

Bayesian Learning - dont remove hypotheses, each example only changes liklihood of one
 prior knowledge can be used to determine probability of a hypothesis
 accomodate hypotheses that have probabilistic predictions - ie. 70% chance it will rain.
 P(h|D) = P(D|h)*P(h)/P(D)
  P(h) = prior prob of h
  P(D) = prob of training data being observed
  P(D|h) = probability of observing D given h
  P(h|D) = posterior probability of h after observing D.......

example
 P(cancer) = .008
 P(!cancer) = .992
 P(+|cancer) = .98
 P(-|cancer) = .02
 P(+|!cancer) = .03
 P(-|!cancer) = .97
  Hmap = P(+|cancer)*P(cancer) = .98*.008 = .0078
  Hmap = P(+|!cancer)*P(!cancer) = .003*.992 = .0298
   liklihood is that you dont have cancer.as that one has higher probability.
   in this case we ignore the P(D) because its a general question?
  P(cancer|+) + P(!cancer|+) = 1
  .0078/P(+) + .0298/P(+) = 1
  P(+) = .0078+.0298=.0376
  p(cancer|+) = P(+|cancer)*P(cancer)/P(+) = .0078/.0376 = .21
  p(!cancer|+) = .79

 p(h1|D) = .4
 p(h2|D) = .3
 p(h3|D) = .3
Bayes optimal classifier
 given a hypothesis set and probabilities of each, outputs most likely output if a given data point.
 consider sum of each for yes and each for no, whichever is higher is the guess.

naive bayes classifier - find the most probable v in V given <a1,a2...an>
 Vmap = argmax for v in V OF P(Vj) PI P(ai|Vj)

example... - V={yes,no}
 Vmap=argmax v in yes,no   P(Vj) times? P(ai|Vj)
 P(yes)*P(outlook=sunny | yes)*P(temp=cool|yes)*P(humidity=high|yes)*P(wind=strong|yes)
 9/14 * 2/9 * 3/9 * 3/9 * 3/9 = .0053
 P(no)*p(outlook=sunny|no) ETC... = 5/14 * 3/5 * 1/5 * 4/5 * 3/5 = .0206 THIS IS THE MORE LIKELY ONE.
 have 9 yes, 14 total, 5 no

NAIVE BAYES ALGORITHM, follows process above.
process - calculate probability of both classifications. then identify values in desired example, calculate the probability of all of those
 values given each classification, then multiply them together. the higher one is the most likely classification for the new data.

sample error (errorS(h) = # misclassifies/# samples
true error (error on future sample w/ respect to distribution D) (errorD(h) = P(misclassify|sample from D)
errirS(h) = (errorS(h)*(1-errorS(h))/n)^(1/2)

motivation - s is subset of D [errorD(La(S)) - errorD(Lb(S))]

split data into k subsets, find error on each, average them together.

like simple bayes, conceptual points, review slides...************************************************************************
 ie unseen error - by sampling data and getting error from those?

evaluation of hypotheses?

concept learning - taking tuples of data to determine a concept.

decision tree - 






