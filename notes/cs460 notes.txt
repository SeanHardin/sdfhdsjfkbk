book for class: Machine Learning Tom Mitchell

grading
projects 40% 4 for 10 each
midterm 30%
non-cumulative final 30%

machine learning - the study of algorithms that improves their performance P at some task T with experience E.
p - desired outcome
T - task
E - given data
performance is related to the desired outcome of an event. ie. win chess, accurately diagnose, identify groups.

supervised learning - training data includes expected output
unsupervised learning - training data does not include expected output.



attributes - features
prediction - class (label)
preprocessing is often needed.
x1, x2 etc - a single training example
X: - set of all possible combinations of features
F:X->Y - target concept
H: - set of all hypotheses that the learner may consider regarding the identity of F
goal of the learner - find h within H such that h(x) == f(x) (the right hypothesis) for all xi in X.
if y is discrete, it is called a classification task
if y is continuous, it is called a regression task

model representations - supervised learning
 decision trees
 neural networks
 support vector machines (SVM)
 Bayesian networks

representations with unsupervised learning - discover pattern in the data
 data without labels?
 clustering
 

probability = # times event occurs/total trials
sample space - set of all possible outcomes of an event
P(a|B) = P(A && B)/P(B)
P(x1,x2,x3,x4) = P(x1|others)P(x2|x3,x4)P(x3|x4)P(x4)
P(A || B) = P(A) + P(B) - P(A,B)
gradient descent better when used with higher dimensional space.
 minimize sum of squared errors..... derive SSE/2, calculate partial derivative with M, get vector.
 something something put a negative on the vector to get the steepest decrease..?
  gradient of F(x,y) = [partial deriv x, partial deriv y] if f(x,y) = x+3y^2, gradient = [1,6y]
   so applying a point to the negative of the gradient, you get closer to minimum.
   
sum of squared error = the sum((actualData - predictedValue)^2)

concept learning - infer a concept by learning from past experiences
inductive learning hypothesis - READ CHAPTER 2 BECAUSE WHAT THE HELL IS THIS.

attributes have various values. introduce new value ? meaning that any value is okay.

given hi and hj, hi is more general than or rqual to hj (hi >= hj) if and only if all possibilities of hj are contained within hi...........
A ? ?   h1
? B ?   h3
h1 !>= h3, h3 !>= h1

find-s algorithm - make a hypothesis to match the first data point, then check every other positive data point and generalize
 the hypothesis with each one. this is a pretty bad algorithm.
 it can predict known failures as positive.
  this algorithm finds the h: <sunny, warm, ?, Strong, ?, ?>, which could be one of 6. itself, one of the 3 pairs, or one of the singles.
 if 2 of 3 options are found, then no hypothesis can explain the data. BUT A ? IS STILL USED TO SHOW IT. THATS STUPID.
 conditions - there are no training errors (aka impossible on real data), there exists a hypothesis that can explains the target concept
 weaknesses - only finds single hypothesis when there could be more, breaks if training errors exist, only finds single hypothesis

candidate elimination algorithm - finds all potential hypotheses that can match the data?
 version space - VSh,d = {h in H | consistent(h,D)} the version space is the set of all hypotheses consistent with training data.
 start with the empty set as the marker for most specific, all ? hypothesis for most general.
  most specific will follow find-S algorithm, 
  most general will include a number of hypotheses, each focusing on a single attribute's value. the possibilities are then limited
   based on what is allowed from the most specific hypothesis. constant checking between them is needed.
   S: <sunny,warm,?,strong,warm,same> is S3
   G: <??????> next data point: <rainy, cold, high, strong, warm, change>
   rainy, cold, change fields are only consistent ones. thus, 3 new hypotheses, the opposite of these 3
   <sunny ?????>, <?warm?????><?????same> is G
    next iteration changes same from s3 into a ?, thus the third hypothesis in G must be removed because it is now inconsistent with the data.
 conditions - there can not be any training errors, can be explained by a set of hypotheses.
 using results - all yes, positive.   majority yes, positive, even, need more data, more fail, negative.

decision tree - learning by inductive inference that is much more widely used than what we covered in concept leaning
 robust to noisy data, search a more expressive hypothesis space.
 learned function is represented as a decision tree, classification label usually discrete values, can be continuous
 used to classify patients by disease, find cause of equipment malfunctions, assess credit risk of loan applicants.
core algorithm known as ID3 (uses entropy and information gain to construct tree)
 'intuition' explains that the more an attribute is able to determine the classification outcome by itself, the higher up it should be.
  information gain is the measure of how well a given attribute determines the classification.
  entropy is used to measure the degree of similarity
   entropy - how pure something is. 
Entropy(s) = -P+*log(P+) - P-*log(P-)    P+ = probability value is positive.  log is base 2.
    min = 0 at only pos/neg.   max = 1 at 50% each
information gain - measures reduction in entropy caused by partitioning training examples based on A
gain(S,A) = Entropy(S) - SIGMA(v in Values(A))(|Sv|*Entropy(Sv)/|S|) this formula is a fucking mess...
          = Entropy(S) - (Sv1/S*Entropy(Sv) + 
 
gain(S,wind)=entropy(S)-(|Sweak|/|S| * entropy(Sweak) + |Sstrong|/|S| * entropy (Sstrong))
 14 entries, strong/yes 3   strong/no 3   weak/yes 6  weak/no 2  
 total strong 6  total weak 8  total yes 9   total no 5

 entropy(S) = -9/14*log(9/14) - 5/14 * log(5/14) = .94ish
 gain = .94 - (6/14 * entropy(Sstrong) + 8/14 * entropy(Sweak))
		      -3/6 * log(3/6) - 3/6*log(3/6)          -6/8*log(6/8) - 2/8*log(2/8)
 = .94 - (6/14*(-3/6 * log(3/6) - 3/6*log(3/6)) + 8/14*( -6/8*log(6/8) - 2/8*log(2/8)))

weakness - doesn't backtrack, but does it need to?

S = sample space of given attribute
A = set potential values of attribute

Entropy(S) = -(prob pos (total yes/total))*log(prob pos)) - (prob neg(total no/total))*log(prob neg))
gain(S,A) = Entropy(S) - {prob S has Ai(total strong/total) * Entropy(S with Ai)} SUMMED FOR ALL Ai
 Entropy(S with Ai) = -(prob+((strong/yes / total strong)3/6)log(prob+(3/6)) -(prob-(3/6)log(prob-(3/6))

entropy(S) = total entropy in table
gain(S,A) = current entropy minus the sum of the entropy found within each possible value of the attribute
 if the entropy within the sum of potential values is significantly smaller than the original entropy, then they should be split up to
  reduce the total entropy in the table. THE VALUE WITH THE HIGHEST GAIN VALUE IS THE BEST OPTION FOR THE ROOT NODE OF A SPLIT.


After going down a branch, S becomes SAi (ie Ssunny on the sunny path)

inductive bias - assumptions regarding the hypothesis and training data allow us to generalize over unseen examples.
bias favors smaller trees?
approximate bias - shorter trees are preferred over larger ones.
Occam's razor - choosing the simplest way to describe why something happens when multiple hypotheses are present.

with training data, overfitting increases accuracy (for obvious reasons)
with real data, accuracy falls once too many nodes are made. peaks at 5, drops around 20, progressively gets worse (in one study)

binning - giving ranges for continuous values to partition them into a manageable number of potential values.
strategies to fill in blank values - 
 choose most common value for that attribute
 choose most common value for that attribute and class label (y/n)
 fill in as probabilities based on previous values.

k nearest neighbor - save training data, classify new instance based on classification of subset of training instances that the new one
 is 'most near' assumes nearness indicates similarity
 measure nearness - d(xi,xj) = (sum((ar*xi - ar*xj)^2) from 1 to n)^(1/2)
essentially, finding the nearest n points, describes the newly added point as the color that is most common among the nearest n previous ones
 in the case of numbers, it would be the mean of them
 k must be odd so that ties dont happen. but they still can if theres 3 types...

 for weighted k nearest neighbor, multiply the distance by the weight
 weight = 1/(d(xq,xi)^2)
as k increases, noise has less of an effect.

advantages - intuitive and easy to understand
 estimate of target function is local and different for every sample
 robust to noisy data, good classificatio if data is suficiently large

disadvantages - cost of classification is high because computation takes place at classification time
 all attributes are used when determining what samples to pick
 determining the optimum k can be tricky.

curse of dimensionality - distance between instances is calculated based on all attributes of the instance. this is in contrast to other learning algorithms
 this can be bad. for instance, with 20 attributes where only 2 apply

Bayesian Learning - dont remove hypotheses, each example only changes liklihood of one
 prior knowledge can be used to determine probability of a hypothesis
 accomodate hypotheses that have probabilistic predictions - ie. 70% chance it will rain.
 P(h|D) = P(D|h)*P(h)/P(D)
  P(h) = prior prob of h
  P(D) = prob of training data being observed
  P(D|h) = probability of observing D given h
  P(h|D) = posterior probability of h after observing D.......

example
 P(cancer) = .008
 P(!cancer) = .992
 P(+|cancer) = .98
 P(-|cancer) = .02
 P(+|!cancer) = .03
 P(-|!cancer) = .97
  Hmap = P(+|cancer)*P(cancer) = .98*.008 = .0078
  Hmap = P(+|!cancer)*P(!cancer) = .003*.992 = .0298
   liklihood is that you dont have cancer.as that one has higher probability.
   in this case we ignore the P(D) because its a general question?
  P(cancer|+) + P(!cancer|+) = 1
  .0078/P(+) + .0298/P(+) = 1
  P(+) = .0078+.0298=.0376
  p(cancer|+) = P(+|cancer)*P(cancer)/P(+) = .0078/.0376 = .21
  p(!cancer|+) = .79

 p(h1|D) = .4
 p(h2|D) = .3
 p(h3|D) = .3
Bayes optimal classifier
 given a hypothesis set and probabilities of each, outputs most likely output if a given data point.
 consider sum of each for yes and each for no, whichever is higher is the guess.

naive bayes classifier - find the most probable v in V given <a1,a2...an>
 Vmap = argmax for v in V OF P(Vj) PI P(ai|Vj)

example... - V={yes,no}
 Vmap=argmax v in yes,no   P(Vj) times? P(ai|Vj)
 P(yes)*P(outlook=sunny | yes)*P(temp=cool|yes)*P(humidity=high|yes)*P(wind=strong|yes)
 9/14 * 2/9 * 3/9 * 3/9 * 3/9 = .0053
 P(no)*p(outlook=sunny|no) ETC... = 5/14 * 3/5 * 1/5 * 4/5 * 3/5 = .0206 THIS IS THE MORE LIKELY ONE.
 have 9 yes, 14 total, 5 no

NAIVE BAYES ALGORITHM, follows process above.
process - calculate probability of both classifications. then identify values in desired example, calculate the probability of all of those
 values given each classification, then multiply them together. the higher one is the most likely classification for the new data.

sample error (errorS(h) = # misclassifies/# samples
true error (error on future sample w/ respect to distribution D) (errorD(h) = P(misclassify|sample from D)
errirS(h) = (errorS(h)*(1-errorS(h))/n)^(1/2)

motivation - s is subset of D [errorD(La(S)) - errorD(Lb(S))]

split data into k subsets, find error on each, average them together.

like simple bayes, conceptual points, review slides...************************************************************************
 ie unseen error - by sampling data and getting error from those?

artificial neural network - the sequence of nodes where the nodes in the next sequence equals some formula of the nodes in the previous
 the formula following the pattern of weight1*val1 + weight2*val2 + etc

perceptron - single step in a neural network.

xor with neural network - 
x1 toh1 w=.5, toh2 w=.5, h1 w=-.8, h1 toh3 w=-2
x2 toh1 w=.5, toh2 w=.5, h2 w=-.3, h2 toh3 w=1.1    h3 w=-1?
0*.5 + 0*.5 - .8 < 0 so 0
0*.5 + 1*.5 - .3 > 0 so 1
x1  x2   h1   h2   h3
0   0    0    0    0+0    0-1   = 0
0   1    0    1    0+1.1  1.1-1 = 1
1   0    0    1    0+1.1  1.1-1 = 1
1   1    1    1    -2+1.1 -.9-1 = 0
feed forward neural network - only pass data forwards.

training a perceptron - find the weight vector w that best explains the data.
 perception training rule - pick random set of Wi, using above weighs, apply the perceptron to training data
  if misclassify training eample, adjust weights. BUT HOW???? THATS THE IMPORTANT PART. repeat.
  new weight = old weight + change in weight
  change in weight = learning rate * (target output - perceptron's output)*xi
  learning rate is decided on from the start, his example uses .01
 Algorithm only works for linearly seperable data.


support vector machine - somehow finds the center of the lines between separated data sets.

Delta rule - gradient descent to minimize error instead of find 'one true line'
 converges to best approximation, introducing us to gradient descent
 (upsidedowntriangle)E(w) = [derivative of w0, deriv w1, etc]
 w = w + change in weight
 change in weight = -learning rate * (tri)E(w)
 ERROR = 1/2 * sum(td-0d)^2
SIMPLIFIED:
 wi = wi + change in weight
 change in weight = (learning rate)(SUM for all data((expected result - predicted result) * respective input value, what was multiplied with weight))
  the change in error is the same as the derivative of the error, so by w = w + change in w where change in w = -learning rate * change in error
   we get a simpler formula to use by plugging in the derivation and combining the equations into one.

Stochastic approximation to gradient descent - variation of delta rule to address the local minimum problem
 change in weight = learning rate*(expected output - predicted output)*xi
 calculation done while calculating individual errors, not after calculating all of them.

perceptron -> computational unit
sigmoid function - G(y) = 1/(1+e^(-y))
derivative(F(x)) = derivative E/dw1, deriv E/dw2...
E = 1/2 * SUM (t - o)^2
E(w) = 1/2 * SUM d in D(SUM k in perceptrons(t(kd) - o(kd))^2)

backpropagation - propagates error back to previous layers..?
 each hidden layer's error is found by 

neural net: 2 inputs, layer 1 has 2 perceptrons, layer 2 has 2 outputs
sigmoid function of net value in each perceptron generates the value multiplied by the next layer's weights.
sigmoid(i1*wh1i1 + i2*wh1i2 + wh1) = h1
o1(1-o1) = derivative of sigmoid
t1-o1 = error of the actual output
  combining them means taking the slope from gradient descent, then multiplying it by the error of the output
error of o1 = o1(1-o1)(t1-o1)
error or h1 = h1(1-h1)(wo1h1*errorOFo1 + wo2h1*errorOFo2) (which is the sum of all connected errors)
w = w + change in w
change in w = learning rate * error of node * output

for output layer
change in weight = learning rate * output * (expected - output) * (1 - output) * respective input value

for hidden layers
change in weight = learning rate * respective input value * output * (1 - output) * SUM all nodes downstream(error * weight)
error = output * (1 - output) * SUM all nodes downstream (error * weight)

support vector machines - drawing lines that intersect the edge points of each classification, then take median between them.

k-means clustering - training examples clustered based on features' similarities          splitting data into clusters.
 centroids of the cluster, and which cluster a training example belongs to.
 start with an initial estimate of k centroids - pick 3 training points and treat as centroids
 until stopping citeria met: assign each data point to nearest centroid based on Euclidian distances, rewrite if other centroid becomes closer
 update centroids: take average of all points in cluster, make that new centroid.

feature scaling - min-max normalization x` = (x-min(x)) / (max(x) - min(x))

Computational Learning theory - focuses on how many training examples are sufficient, how many errors before result acceptable
 sample size - PAC framework - Probably approximately correct
  learning setting - X all instances, C(X) target function, H hyp space, h hyp, L outputs best h
  what is the minimum # samples needed assuming that learner does not necessarily come up with 0 error hypothesis, but rather small error.
   learner doesn't need to succeed on every training example
m >= 1/(2E^2) * (ln|H| + ln(1/8)) = min # samples.
 in case of old, not old, ?. then there are 3^n hyps where n is number of boolean literals.
 with 2 literals, 1/e * nln(3)+ln(1/e)....wtf?

reinforced learning - learning by maximizing successes
 Markov something
  S: set of states
  A: set of actions
   you are an s IN S, you do an action a IN A. you get rewarded by the result, amount changing by maximizing reward.
    example - robot to charging station. each room transfer, multiply reward received in future by .9, more mistakes, smaller reward.
     only reward on final room, optimal path involves fewest rooms traveled.
   Q(s,a) = r(s,a) + gamma * vSTAR(8thing(s,a))
   r(s,a) = reward received by doing action a from state s
   8thing(s,a)= the state resulting from applying action a to state s
   gamma = value reward is reduced by following optimal policy?

 Q(s,a) = r(s,a) + gamma * vSTAR(8thing(s,a))
 Q(s,a) - the new value to set r(s,a) to
 r(s,a) - the reward for performing action a at state s
 gamma - the weighted loss every move taken
 vstar(8thing(s,a)) - the max of potential actions for state s

REVIEW STUFF, AKA WHAT HE THINKS IS IMPORTANT ENOUGH TO RE-LECTURE ON.

majority of 4 chapters?

ANNs everything in slides is fair game

Q learning

vapnic chervoneskis dimension   why? to put some quantity in training dimension?








find s
candidate elimination
k nearest neighbor
naive bayes
normal bayes
bayes optimal classifier
decision tree
 entropy, gain
Perceptron, delta rule (gradient descent)
backpropagation
k means clustering
PAC
SVMs
markov chains/Q learning/ reinforcement learning













